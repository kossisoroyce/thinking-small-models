\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Listings setup for code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    xleftmargin=2em
}

% Title
\title{\textbf{Thinking Small Models: Multi-Stage Reasoning for Interpretable Machine Learning}}
\author{Kossiso Royce\\
Electric Sheep Africa\\
\texttt{Kossi@electricsheep.africa}}
\date{}

\begin{document}

\maketitle

% Abstract
\begin{abstract}
Large language models (LLMs) have demonstrated reasoning capabilities through techniques like chain-of-thought prompting and self-correction. We ask: can similar patterns be applied to traditional ML models? We present \textbf{Thinking XGBoost}, a multi-stage XGBoost pipeline that structures predictions through specialized heads, hybrid aggregation, and critic-triggered refinement. Our main contribution is the \textbf{Reasoning Quality Score (RQS)}, a formal framework with five metrics for evaluating interpretability in non-LLM models: Decomposability, Self-Correction, Reasoning Coherence, Explanation Faithfulness, and Graceful Degradation. On a synthetic fraud detection dataset, we demonstrate that (1) critic-based self-correction achieves F1=0.74 in detecting aggregator errors, and (2) the RQS framework provides measurable targets for reasoning transparency. The architecture itself builds on established ensemble techniques (stacking, cascades) but offers a concrete testbed for the proposed evaluation framework. This work represents an initial inquiry into structured reasoning for traditional ML; while early results show a $\sim$2\% AUC trade-off, this proof-of-concept demonstrates promising directions that we continue to refine.
\end{abstract}

\textbf{Keywords:} Interpretable Machine Learning, XGBoost, Reasoning, Self-Correction, Explainability, Fraud Detection

\section{Introduction}

\subsection{Motivation}

The success of large language models in complex reasoning tasks has sparked interest in understanding \textit{how} models arrive at decisions, not just \textit{what} they predict. Techniques like chain-of-thought prompting \citep{wei2022chain}, self-consistency \citep{wang2023selfconsistency}, and GRPO \citep{shao2024deepseekmath} have enabled LLMs to decompose problems, verify their work, and self-correct errors.

Meanwhile, traditional ML models---XGBoost, random forests, neural networks---remain largely opaque. While feature importance and SHAP values provide post-hoc explanations, they lack the structured reasoning traces that make LLM outputs interpretable. This gap is particularly problematic in high-stakes domains like finance, healthcare, and legal systems, where regulators increasingly demand not just accurate predictions, but \textit{explainable} ones.

\subsection{Research Questions}

We address three questions:
\begin{enumerate}
    \item \textbf{Can traditional ML models ``think''?} Can we adapt LLM reasoning patterns---multi-step decomposition, aggregation, self-correction---to gradient boosting models?
    \item \textbf{How do we measure reasoning quality?} LLMs are evaluated on chain-of-thought coherence and self-consistency. What equivalent metrics exist for small models?
    \item \textbf{Does thinking help?} Beyond interpretability, does structured reasoning improve model robustness or error detection?
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{Reasoning Quality Score (RQS) Framework} (Primary): Five formal metrics with mathematical definitions for evaluating ``reasoning'' in non-LLM models. This addresses the lack of standardized evaluation for structured interpretability.
    \item \textbf{Critic-based Self-Correction}: Empirical demonstration that an XGBoost critic can detect aggregator errors with F1=0.74, enabling selective refinement on 5.4\% of predictions.
    \item \textbf{Reference Implementation}: A 4-stage pipeline (heads $\rightarrow$ aggregator $\rightarrow$ critic $\rightarrow$ refiner) that serves as a testbed for the RQS framework. The architecture builds on stacking \citep{wolpert1992stacked} and cascades \citep{viola2001rapid}.
\end{enumerate}

\subsection{What Does ``Thinking'' Mean for Small Models?}

We use ``thinking'' as a functional analogy, not a cognitive claim. In LLMs, ``thinking'' typically refers to:
\begin{itemize}
    \item \textbf{Decomposition}: Breaking problems into intermediate steps
    \item \textbf{Aggregation}: Combining evidence toward a conclusion
    \item \textbf{Self-correction}: Detecting and revising errors
\end{itemize}

We propose that a traditional ML model exhibits ``thinking'' behavior if it satisfies three operational criteria:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Criterion} & \textbf{LLM Equivalent} & \textbf{Small Model Equivalent} \\
\midrule
Decomposable & Chain-of-thought steps & Interpretable sub-predictions (heads) \\
Aggregative & Combining reasoning paths & Explicit aggregation with traceable weights \\
Self-correcting & Verifier/critic models & Error-detection triggering refinement \\
\bottomrule
\end{tabular}
\caption{Mapping LLM reasoning concepts to small models.}
\label{tab:thinking-criteria}
\end{table}

\textbf{Formal Definition}: A model $M$ ``thinks'' if:
\begin{enumerate}
    \item Its prediction can be decomposed into $K \geq 2$ interpretable sub-predictions
    \item The aggregation mechanism is explicit and traceable
    \item A self-correction mechanism exists with $F1 > 0.5$ on error detection
\end{enumerate}

This is deliberately minimal. We do not claim small models reason like humans or LLMs---only that they can exhibit \textit{structured transparency} that mimics the functional properties we value in LLM reasoning. The RQS framework operationalizes this definition into measurable metrics.

\section{Related Work}

\subsection{Interpretable Machine Learning}

Traditional interpretability methods focus on post-hoc explanations:
\begin{itemize}
    \item \textbf{Feature Importance}: Measures variable contributions \citep{breiman2001random}
    \item \textbf{SHAP Values}: Game-theoretic feature attributions \citep{lundberg2017unified}
    \item \textbf{LIME}: Local linear approximations \citep{ribeiro2016should}
\end{itemize}

These approaches explain \textit{what} features matter but not \textit{how} the model reasons through them.

\subsection{LLM Reasoning}

Recent advances in LLM reasoning include:
\begin{itemize}
    \item \textbf{Chain-of-Thought (CoT)}: Decomposing problems into steps \citep{wei2022chain}
    \item \textbf{Self-Consistency}: Sampling multiple reasoning paths \citep{wang2023selfconsistency}
    \item \textbf{GRPO}: Group Relative Policy Optimization for reasoning \citep{shao2024deepseekmath}
    \item \textbf{Critics and Verifiers}: Separate models that check outputs \citep{cobbe2021training}
\end{itemize}

Our work adapts these concepts to gradient boosting.

\subsection{Ensemble Methods with Structure}

Prior work on structured ensembles includes:
\begin{itemize}
    \item \textbf{Stacking}: Meta-learners over base predictions \citep{wolpert1992stacked}
    \item \textbf{Cascades}: Sequential refinement \citep{viola2001rapid}
    \item \textbf{Mixture of Experts}: Gated expert selection \citep{jacobs1991adaptive}
\end{itemize}

Our architecture combines these ideas with explicit reasoning traces and critic-based self-correction, providing a testbed for evaluating reasoning quality.

\section{Methodology}

\subsection{Problem Setting}

We consider binary classification for fraud detection with features $\mathbf{x} \in \mathbb{R}^d$ and label $y \in \{0, 1\}$. Features are partitioned into semantic groups $G = \{G_1, \ldots, G_K\}$ representing distinct fraud dimensions (e.g., transaction amount, velocity, location).

\subsection{Architecture Overview}

The Thinking XGBoost pipeline consists of four stages:

\begin{lstlisting}
Stage 1: Reasoning Heads     h_k(x) -> [0,1] for k in {1,...,K}
Stage 2: Hybrid Aggregator   a(h_1,...,h_K) -> [0,1]
Stage 3: Critic              c(a, h_1,...,h_K) -> [0,1]  
Stage 4: Refiner             r(x, h_1,...,h_K) -> [0,1]
\end{lstlisting}

\textbf{Final prediction:}
\begin{equation}
\hat{y} = \begin{cases} r(\mathbf{x}) & \text{if } c(\cdot) > \tau \\ a(\cdot) & \text{otherwise} \end{cases}
\end{equation}
where $\tau$ is the critic threshold.

\subsection{Stage 1: Reasoning Heads}

Each reasoning head $h_k$ is an XGBoost classifier trained on feature subset $G_k$:
\begin{equation}
h_k(\mathbf{x}) = \text{XGBoost}_k(\mathbf{x}_{G_k})
\end{equation}

Heads specialize in detecting fraud signals within their domain:
\begin{itemize}
    \item \textbf{Amount head}: Transaction amount anomalies
    \item \textbf{Velocity head}: Unusual transaction frequency
    \item \textbf{Merchant head}: Merchant risk factors
    \item \textbf{Location head}: Geographic signals
    \item \textbf{Device head}: Device/channel risk
    \item \textbf{Time head}: Temporal patterns
\end{itemize}

Each head outputs a fraud probability $h_k(\mathbf{x}) \in [0,1]$ and a binary signal $s_k = \mathbb{1}[h_k(\mathbf{x}) > 0.5]$.

\textbf{Learned Weights}: Each head receives a weight $w_k$ proportional to its training AUC:
\begin{equation}
w_k = \frac{\text{AUC}_k}{\sum_j \text{AUC}_j}
\end{equation}

\subsection{Stage 2: Hybrid Aggregator}

The aggregator combines head outputs using a hybrid approach:
\begin{equation}
a(\mathbf{h}) = \alpha \cdot \underbrace{\sum_k w_k h_k}_{\text{Weighted Average}} + (1-\alpha) \cdot \underbrace{f_{\text{XGB}}(\mathbf{h}, \mathbf{h} \otimes \mathbf{h})}_{\text{XGBoost with Interactions}}
\end{equation}
where $\alpha = 0.6$ (blend ratio) and $\mathbf{h} \otimes \mathbf{h}$ includes pairwise products $h_i \cdot h_j$ for interaction terms.

The weighted average component provides \textbf{faithfulness}---changes in head scores directly affect the output. The XGBoost component captures \textbf{non-linear interactions} between heads.

\subsection{Stage 3: Critic}

The critic model predicts when the aggregator will be wrong:
\begin{equation}
c(\cdot) = \text{XGBoost}_{\text{critic}}(\mathbf{z})
\end{equation}
where $\mathbf{z}$ includes:
\begin{itemize}
    \item Aggregator prediction $a(\mathbf{h})$
    \item Aggregator confidence $|a - 0.5| \times 2$
    \item Individual head scores $h_k$
    \item Head-aggregator deviation $|h_k - a|$
    \item Head disagreement (std, range)
    \item Method disagreement $|\text{weighted\_avg} - \text{xgb\_pred}|$
\end{itemize}

\textbf{Training}: The critic is trained on cross-validation errors of the aggregator:
\begin{equation}
y_{\text{critic}} = \mathbb{1}[\hat{y}_{\text{CV}} \neq y_{\text{true}}]
\end{equation}

Using CV predictions avoids the problem of training on zero errors when the aggregator overfits.

\textbf{Threshold Selection}: We optimize for F1 score on error detection:
\begin{equation}
\tau^* = \arg\max_\tau F1(\mathbb{1}[c(\cdot) > \tau], y_{\text{critic}})
\end{equation}

\subsection{Stage 4: Refiner}

The refiner is a stronger XGBoost model trained with emphasis on hard cases:
\begin{equation}
r(\mathbf{x}) = \text{XGBoost}_{\text{refiner}}(\mathbf{x}, \mathbf{h})
\end{equation}

\textbf{Training}: Samples where the aggregator was wrong receive higher weight:
\begin{equation}
w_i = \begin{cases} 8.0 & \text{if aggregator wrong} \\ 1.0 & \text{otherwise} \end{cases}
\end{equation}

The refiner uses both original features $\mathbf{x}$ and head outputs $\mathbf{h}$, providing maximum information for difficult cases.

\subsection{Reasoning Trace}

For each prediction, the pipeline outputs a structured reasoning trace:

\begin{lstlisting}[language=XML]
<REASONING>
  amount       risk: 0.374
  velocity     risk: 0.760
  merchant     risk: 0.715
  location     risk: 0.596
  device       risk: 0.429
  time         risk: 0.484
  --------------------
  Weighted avg:     0.568
  XGBoost pred:     0.815
  Blended:          0.666
  Critic score:     0.669
  [!] REFINEMENT TRIGGERED
</REASONING>

<SOLUTION>
  Probability: 0.054
  Decision:    LEGITIMATE
</SOLUTION>
\end{lstlisting}

This trace mirrors LLM chain-of-thought outputs, providing interpretable intermediate steps.

\section{Reasoning Quality Score (RQS) Framework}

\subsection{Motivation}

LLMs are evaluated on reasoning quality through metrics like chain-of-thought coherence and self-consistency. Traditional ML models lack equivalent evaluation frameworks. We propose the \textbf{Reasoning Quality Score (RQS)} with five metrics.

\subsection{Metric Definitions}

\subsubsection{Decomposability (D)}

\textbf{Definition}: The degree to which the final prediction can be attributed to interpretable sub-components.
\begin{equation}
D = 1 - \frac{\text{Var}(\hat{y} - \bar{h})}{\text{Var}(\hat{y})}
\end{equation}
where $\bar{h} = \frac{1}{K}\sum_k h_k$ is the mean head prediction.

\textbf{Interpretation}: $D = 1$ means heads fully explain the prediction; $D = 0$ means heads explain nothing.

\textbf{Target}: $D \geq 0.70$

\subsubsection{Self-Correction (SC)}

\textbf{Definition}: The model's ability to identify its own errors.
\begin{equation}
SC = F1(\text{critic\_flags}, \text{actual\_errors})
\end{equation}

\textbf{Interpretation}: High SC means the critic accurately identifies when the aggregator will be wrong.

\textbf{Target}: $SC \geq 0.30$

\subsubsection{Reasoning Coherence (RC)}

\textbf{Definition}: Consistency between intermediate reasoning signals and final decision.
\begin{equation}
RC = \frac{1}{K} \sum_k |\rho(h_k, \hat{y})|
\end{equation}
where $\rho$ is Pearson correlation.

\textbf{Interpretation}: High RC means head scores correlate with final predictions.

\textbf{Target}: $RC \geq 0.50$

\subsubsection{Explanation Faithfulness (EF)}

\textbf{Definition}: Do the stated reasons actually influence the prediction?

For each head $k$, we perturb its input features and measure:
\begin{equation}
EF_k = \rho(\Delta h_k, \Delta \hat{y})
\end{equation}
\begin{equation}
EF = \frac{1}{K} \sum_k \max(0, EF_k)
\end{equation}

\textbf{Interpretation}: High EF means perturbing a head's inputs proportionally changes the final output.

\textbf{Target}: $EF \geq 0.60$

\subsubsection{Graceful Degradation (GD)}

\textbf{Definition}: When the model is wrong, are errors concentrated in identifiable dimensions?
\begin{equation}
GD = \frac{H(\mathbf{p}_{\text{error}})}{\log K}
\end{equation}
where $H$ is entropy and $\mathbf{p}_{\text{error}}$ is the distribution of which heads had extreme predictions on errors.

\textbf{Interpretation}: Low GD means errors are concentrated (easier to debug); high GD means errors are spread out.

\textbf{Target}: $GD \leq 0.50$

\subsection{Composite Score}

\begin{equation}
RQS = 0.20 \cdot D + 0.25 \cdot SC + 0.20 \cdot RC + 0.25 \cdot EF + 0.10 \cdot (1 - GD)
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{RQS Range} & \textbf{Interpretation} \\
\midrule
0.8 -- 1.0 & Excellent reasoning transparency \\
0.6 -- 0.8 & Good reasoning transparency \\
0.4 -- 0.6 & Moderate reasoning transparency \\
0.0 -- 0.4 & Poor reasoning transparency \\
\bottomrule
\end{tabular}
\caption{RQS interpretation scale.}
\label{tab:rqs-scale}
\end{table}

\section{Experiments}

\subsection{Dataset}

We use a synthetic fraud detection dataset with 30,000 transactions:
\begin{itemize}
    \item \textbf{Fraud rate}: 8\%
    \item \textbf{Features}: 18 (grouped into 6 semantic categories)
    \item \textbf{Patterns}: Normal legitimate, suspicious legitimate (gray zone), obvious fraud, subtle fraud, mixed-signal fraud
\end{itemize}

The synthetic dataset enables controlled evaluation of reasoning quality with known ground truth patterns.

\subsection{Experimental Setup}

\begin{itemize}
    \item \textbf{Train/Test Split}: 80/20 with stratification
    \item \textbf{Random Seed}: 42 (fixed for reproducibility)
    \item \textbf{Baseline}: Standard XGBoost (100 trees, depth 6)
    \item \textbf{Metrics}: ROC-AUC, Classification Report, RQS
\end{itemize}

\subsection{Results}

\subsubsection{Traditional Metrics}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Precision (Fraud)} & \textbf{Recall (Fraud)} \\
\midrule
Baseline XGBoost & 0.994 & 0.89 & 0.85 \\
Thinking Pipeline & 0.976 & 0.85 & 0.82 \\
\bottomrule
\end{tabular}
\caption{Traditional performance metrics. The Thinking Pipeline trades $\sim$2\% AUC for full reasoning transparency.}
\label{tab:traditional-metrics}
\end{table}

\subsubsection{Reasoning Quality Score}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Score} & \textbf{Target} & \textbf{Status} \\
\midrule
\textbf{RQS} & \textbf{0.50} & $>$0.60 & --- \\
Decomposability & 0.77 & $>$0.70 & \checkmark \\
Self-Correction & 0.33 & $>$0.30 & \checkmark \\
Coherence & 0.57 & $>$0.50 & \checkmark \\
Faithfulness & 0.57 & $>$0.60 & $\times$ \\
Graceful Degradation & 0.91 & $<$0.50 & $\times$ \\
\bottomrule
\end{tabular}
\caption{Reasoning Quality Score results. Three of five metrics meet their targets.}
\label{tab:rqs-results}
\end{table}

\subsubsection{Self-Correction Analysis}

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Samples Refined & 326 / 6000 (5.4\%) \\
Critic F1 & 0.74 \\
Decisions Changed & 312 \\
\bottomrule
\end{tabular}
\caption{Self-correction analysis. The critic successfully identifies uncertain predictions.}
\label{tab:self-correction}
\end{table}

\subsection{Ablation Studies}

\subsubsection{Blend Ratio Impact}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Blend Ratio ($\alpha$)} & \textbf{RQS} & \textbf{Faithfulness} & \textbf{AUC} \\
\midrule
0.0 (XGBoost only) & 0.43 & 0.32 & 0.988 \\
0.4 & 0.46 & 0.42 & 0.982 \\
\textbf{0.6} & \textbf{0.50} & \textbf{0.57} & \textbf{0.976} \\
0.8 & 0.49 & 0.68 & 0.968 \\
1.0 (Weighted avg only) & 0.28 & 0.53 & 0.963 \\
\bottomrule
\end{tabular}
\caption{Blend ratio ablation. The optimal blend ratio (0.6) balances faithfulness and predictive power.}
\label{tab:blend-ablation}
\end{table}

\subsubsection{Component Contributions}

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{RQS} \\
\midrule
Heads only & 0.32 \\
+ Aggregator & 0.38 \\
+ Critic & 0.44 \\
+ Refiner & 0.50 \\
\bottomrule
\end{tabular}
\caption{Component contributions. Each stage contributes to overall reasoning quality.}
\label{tab:component-ablation}
\end{table}

\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{LLM reasoning patterns transfer to small models}: Multi-stage decomposition, aggregation, and self-correction improve interpretability without catastrophic accuracy loss.
    \item \textbf{Self-correction works}: The critic achieves F1=0.74 in detecting aggregator errors, enabling selective refinement.
    \item \textbf{Trade-offs exist}: Higher faithfulness (explaining predictions through heads) reduces predictive power. The hybrid aggregator balances this trade-off.
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Graceful Degradation (GD = 0.91)}: Errors spread across multiple heads rather than concentrating in identifiable dimensions. This appears to be a fundamental limitation of multi-dimensional fraud patterns.
    \item \textbf{Faithfulness gap (EF = 0.57 vs 0.60 target)}: The XGBoost component of the aggregator dampens individual head contributions.
    \item \textbf{Synthetic data}: Results on real-world data may differ due to noisier patterns and feature correlations.
\end{enumerate}

\subsection{Practical Implications}

\begin{enumerate}
    \item \textbf{Regulatory Compliance}: The reasoning trace satisfies explainability requirements in regulated industries.
    \item \textbf{Human-in-the-Loop}: Analysts can quickly triage alerts by examining which dimensions flagged risk.
    \item \textbf{Debugging}: When models fail, the trace identifies which reasoning dimension was wrong.
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
    \item \textbf{Real-world validation}: Apply to production fraud detection systems.
    \item \textbf{Other domains}: Healthcare diagnosis, credit risk, content moderation.
    \item \textbf{Metric refinement}: Develop causal faithfulness measures.
    \item \textbf{Architecture improvements}: Attention-based aggregation, learned critic thresholds.
\end{enumerate}

\section{Conclusion}

We introduced the \textbf{Reasoning Quality Score (RQS)} framework---five metrics for evaluating structured interpretability in traditional ML models. This addresses a gap in how we measure ``reasoning'' outside of LLMs.

As a testbed, we built Thinking XGBoost, a 4-stage pipeline combining established techniques (stacking, cascades) with critic-based self-correction. Key empirical findings:
\begin{itemize}
    \item The critic achieves F1=0.74 in detecting aggregator errors
    \item 3/5 RQS targets met (Decomposability, Self-Correction, Coherence)
    \item $\sim$2\% AUC trade-off for full reasoning transparency
\end{itemize}

\textbf{Limitations}: Faithfulness (0.57) and Graceful Degradation (0.91) remain below targets. The evaluation uses synthetic data only.

This work represents an initial inquiry into structured reasoning for traditional ML. While our proof-of-concept shows a modest $\sim$2\% AUC trade-off, the results demonstrate that LLM-inspired reasoning patterns can be meaningfully adapted to small models. We are actively refining the approach to improve Faithfulness and reduce Graceful Degradation.

We hope the RQS framework provides a starting point for standardized evaluation of interpretable ML systems, independent of the specific architecture used.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L. (2001).
\newblock Random forests.
\newblock \textit{Machine Learning}, 45(1):5--32.

\bibitem[Cobbe et~al.(2021)]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. (2021).
\newblock Training verifiers to solve math word problems.
\newblock \textit{arXiv preprint arXiv:2110.14168}.

\bibitem[Jacobs et~al.(1991)]{jacobs1991adaptive}
Jacobs, R.~A., Jordan, M.~I., Nowlan, S.~J., and Hinton, G.~E. (1991).
\newblock Adaptive mixtures of local experts.
\newblock \textit{Neural Computation}, 3(1):79--87.

\bibitem[Lundberg and Lee(2017)]{lundberg2017unified}
Lundberg, S.~M. and Lee, S.-I. (2017).
\newblock A unified approach to interpreting model predictions.
\newblock In \textit{Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Ribeiro et~al.(2016)]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C. (2016).
\newblock ``Why should I trust you?'': Explaining the predictions of any classifier.
\newblock In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)}.

\bibitem[Shao et~al.(2024)]{shao2024deepseekmath}
Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y.~K., Wu, Y., and Guo, D. (2024).
\newblock DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.
\newblock \textit{arXiv preprint arXiv:2402.03300}.

\bibitem[Viola and Jones(2001)]{viola2001rapid}
Viola, P. and Jones, M. (2001).
\newblock Rapid object detection using a boosted cascade of simple features.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}.

\bibitem[Wang et~al.(2023)]{wang2023selfconsistency}
Wang, X., Wei, J., Schuurmans, D., Le, Q.~V., Chi, E.~H., Narang, S., Chowdhery, A., and Zhou, D. (2023).
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \textit{International Conference on Learning Representations (ICLR)}.

\bibitem[Wei et~al.(2022)]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D. (2022).
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In \textit{Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Wolpert(1992)]{wolpert1992stacked}
Wolpert, D.~H. (1992).
\newblock Stacked generalization.
\newblock \textit{Neural Networks}, 5(2):241--259.

\end{thebibliography}

\appendix

\section{Hyperparameters}

\begin{table}[H]
\centering
\begin{tabular}{llc}
\toprule
\textbf{Component} & \textbf{Parameter} & \textbf{Value} \\
\midrule
Reasoning Heads & n\_estimators & 60 \\
                & max\_depth & 5 \\
                & learning\_rate & 0.1 \\
\midrule
XGBoost Aggregator & n\_estimators & 30 \\
                   & max\_depth & 4 \\
                   & learning\_rate & 0.1 \\
\midrule
Critic & n\_estimators & 100 \\
       & max\_depth & 5 \\
       & learning\_rate & 0.05 \\
\midrule
Refiner & n\_estimators & 180 \\
        & max\_depth & 8 \\
        & learning\_rate & 0.03 \\
\midrule
Blend Ratio & $\alpha$ & 0.6 \\
Error Weight & $w_{\text{error}}$ & 8.0 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters used in the Thinking XGBoost pipeline.}
\label{tab:hyperparameters}
\end{table}

\section{Feature Groups}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Group} & \textbf{Features} \\
\midrule
Amount & amount, avg\_amount\_30d, amount\_vs\_avg\_ratio \\
Velocity & txn\_count\_1h, txn\_count\_24h, velocity\_score \\
Merchant & merchant\_category\_risk, merchant\_age\_days, merchant\_txn\_volume, merchant\_risk\_score \\
Location & distance\_from\_home, is\_foreign\_country, country\_risk\_score, location\_risk \\
Device & is\_new\_device, failed\_attempts\_24h \\
Time & hour\_of\_day, day\_of\_week \\
\bottomrule
\end{tabular}
\caption{Feature groups used for reasoning heads.}
\label{tab:feature-groups}
\end{table}

\end{document}
